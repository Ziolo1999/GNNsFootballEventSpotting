{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "\n",
    "from data_management.DataManager import CALFData, collateGCN\n",
    "import numpy as np\n",
    "import torch \n",
    "from Model import ContextAwareModel, SpottingModel, SegmentationModel\n",
    "from helpers.loss import ContextAwareLoss, SpottingLoss\n",
    "from modules.train import trainer\n",
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "from modules.Visualiser import collateVisGCN, Visualiser\n",
    "import seaborn as sns\n",
    "from helpers.classes import EVENT_DICTIONARY_V2_ALIVE as event_enc\n",
    "from helpers.classes import get_K_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    # DATA\n",
    "    chunk_size = 60\n",
    "    batch_size = 32\n",
    "    input_channel = 13\n",
    "    annotation_nr = 10\n",
    "    receptive_field = 12\n",
    "    fps = 5\n",
    "    K_parameters = get_K_params(chunk_size)\n",
    "    focused_annotation = None\n",
    "    generate_augmented_data = True\n",
    "    class_split = \"alive\"\n",
    "    generate_artificial_targets = False\n",
    "    \n",
    "    # TRAINING\n",
    "    chunks_per_epoch = 1824\n",
    "    lambda_coord=5.0\n",
    "    lambda_noobj=0.5\n",
    "    patience=25\n",
    "    LR=1e-03\n",
    "    max_epochs=180\n",
    "    GPU=0 \n",
    "    max_num_worker=1\n",
    "    loglevel='INFO'\n",
    "    \n",
    "    # SEGMENTATION MODULE\n",
    "    feature_multiplier=1\n",
    "    backbone_player = \"GCN\"\n",
    "    load_weights=None\n",
    "    model_name=\"Testing_Model\"\n",
    "    dim_capsule=16\n",
    "    vocab_size=64\n",
    "    pooling=\"NetVLAD\"\n",
    "\n",
    "    # SPOTTING MODULE\n",
    "    sgementation_path = None\n",
    "    freeze_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data preprocessing:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data preprocessing: 100%|██████████| 1/1 [00:17<00:00, 17.99s/it]\n",
      "Get labels & features: 100%|██████████| 4/4 [00:31<00:00,  7.99s/it]\n"
     ]
    }
   ],
   "source": [
    "args = Args\n",
    "collate_fn = collateGCN\n",
    "\n",
    "train_dataset = CALFData(split=\"train\", args=args)\n",
    "validation_dataset = CALFData(split=\"validate\", args=args)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "            batch_size=args.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "validate_loader = torch.utils.data.DataLoader(validation_dataset,\n",
    "            batch_size=args.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model = SegmentationModel(args=args)\n",
    "criterion = ContextAwareLoss(K=train_dataset.K_parameters)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.LR, \n",
    "                            betas=(0.9, 0.999), eps=1e-07, \n",
    "                            weight_decay=0, amsgrad=False)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', verbose=True, patience=args.patience)\n",
    "\n",
    "losses = trainer(train_loader, validate_loader,\n",
    "                    model, optimizer, scheduler, \n",
    "                    criterion,\n",
    "                    model_name=args.model_name,\n",
    "                    max_epochs=args.max_epochs, \n",
    "                    save_dir=f\"models/CALF_NetVLAD_GCN.pth.tar\")\n",
    "\n",
    "del train_dataset, validation_dataset, train_loader, validate_loader\n",
    "\n",
    "with open(f'results/CALF_NetVLAD_GCN.pkl', 'wb') as file:\n",
    "    pickle.dump(losses, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,rep = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN output size:  torch.Size([32, 152, 300, 1])\n",
      "NetVLAD output size:  torch.Size([32, 10, 300, 1])\n"
     ]
    }
   ],
   "source": [
    "result = model(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DissEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
