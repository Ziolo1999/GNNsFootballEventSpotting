{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "from DataManager import CALFData, collateGCN\n",
    "import numpy as np\n",
    "import torch \n",
    "from Model import ContextAwareModel\n",
    "from helpers.loss import ContextAwareLoss, SpottingLoss\n",
    "from train import trainer\n",
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "from Visualiser import collateVisGCN, Visualiser, VisualiseDataset\n",
    "import seaborn as sns\n",
    "from helpers.classes import EVENT_DICTIONARY_V2_ALIVE as event_enc\n",
    "from helpers.classes import get_K_params\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    receptive_field = 12\n",
    "    fps = 5\n",
    "    chunks_per_epoch = 1824\n",
    "    class_split = \"alive\"\n",
    "    chunk_size = 60\n",
    "    batch_size = 32\n",
    "    input_channel = 13\n",
    "    feature_multiplier=1\n",
    "    backbone_player = \"GCN\"\n",
    "    max_epochs=180\n",
    "    load_weights=None\n",
    "    model_name=\"Testing_Model\"\n",
    "    dim_capsule=16\n",
    "    lambda_coord=5.0\n",
    "    lambda_noobj=0.5\n",
    "    patience=25\n",
    "    LR=1e-03\n",
    "    GPU=0 \n",
    "    max_num_worker=1\n",
    "    loglevel='INFO'\n",
    "    annotation_nr = 1\n",
    "    K_parameters = get_K_params(chunk_size)\n",
    "    focused_annotation = \"Duel\"\n",
    "    generate_augmented_data = True\n",
    "    sgementation_path = \"models/gridsearch5.pth.tar\"\n",
    "    freeze_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args\n",
    "collate_fn = collateGCN\n",
    "list_anns = list(event_enc.keys())\n",
    "\n",
    "for ann in list_anns:\n",
    "    print(f\"\\n {ann}\")\n",
    "    args.focused_annotation = ann\n",
    "    \n",
    "    # Read data for specific annotation\n",
    "    train_dataset = CALFData(split=\"train\", args=args)\n",
    "    validation_dataset = CALFData(split=\"validate\", args=args)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                batch_size=args.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    validate_loader = torch.utils.data.DataLoader(validation_dataset,\n",
    "                batch_size=args.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    # Load pre-trained model and adjust it\n",
    "    model = torch.load(args.sgementation_path)\n",
    "    model.num_classes = 1\n",
    "    model.conv_seg = nn.Conv2d(in_channels=152, out_channels=model.dim_capsule, kernel_size=(model.kernel_seg_size,1))\n",
    "\n",
    "    criterion_segmentation = ContextAwareLoss(K=train_dataset.K_parameters)\n",
    "    criterion_spotting = SpottingLoss(lambda_coord=args.lambda_coord, lambda_noobj=args.lambda_noobj)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.LR, \n",
    "                                betas=(0.9, 0.999), eps=1e-07, \n",
    "                                weight_decay=0, amsgrad=False)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', verbose=True, patience=args.patience)\n",
    "\n",
    "    losses = trainer(train_loader, validate_loader,\n",
    "                        model, optimizer, scheduler, \n",
    "                        [criterion_segmentation, criterion_spotting], \n",
    "                        [args.loss_weight_segmentation, args.loss_weight_detection],\n",
    "                        model_name=args.model_name,\n",
    "                        max_epochs=args.max_epochs, evaluation_frequency=args.evaluation_frequency,\n",
    "                        save_dir=f\"models/finetuned_{ann}.pth.tar\")\n",
    "\n",
    "    with open(f'results/finetuned_{ann}.pkl', 'wb') as file:\n",
    "        pickle.dump(losses, file)\n",
    "    \n",
    "    del train_dataset,validation_dataset,train_loader, validate_loader,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DissEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
